{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    path = path.strip()\n",
    "    path = path.rstrip(\"\\\\\")\n",
    "    isExists = os.path.exists(path)\n",
    "    # 判断结果\n",
    "    if not isExists:\n",
    "        # 如果不存在则创建目录\n",
    "        # 创建目录操作函数\n",
    "        os.makedirs(path) \n",
    "        print(path+' 创建成功')\n",
    "        return True\n",
    "    else:\n",
    "        # 如果目录存在则不创建，并提示目录已存在\n",
    "        #print(path+' 目录已存在')\n",
    "        return False\n",
    "\n",
    "\n",
    "# 检查GPU\n",
    "def GPU():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 模型构建\n",
    "class VAECNN(keras.Model):\n",
    "    def __init__(self, z_dim, L, opt, the_type=0):\n",
    "        super(VAECNN, self).__init__()\n",
    "        # 编码器\n",
    "        self.en1 = layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1))\n",
    "        self.en2 = layers.MaxPooling2D((2, 2))\n",
    "        self.en3 = layers.Conv2D(16, (5, 5), activation='relu')\n",
    "        self.en4 = layers.MaxPooling2D((2, 2))\n",
    "        self.en5 = layers.Conv2D(120, (4, 4), activation='relu')\n",
    "        self.en6 = layers.Flatten()\n",
    "        self.en7 = layers.Dense(84, activation='relu')\n",
    "        # 均值\n",
    "        self.en8 = layers.Dense(z_dim)\n",
    "        # 方差\n",
    "        self.en9 = layers.Dense(z_dim)\n",
    "\n",
    "        # 解码器\n",
    "        self.de1 = layers.Dense(84)\n",
    "        self.de2 = layers.Dense(120)\n",
    "        self.de3 = layers.Conv2DTranspose(16, (4, 4), activation='relu')\n",
    "        self.de4 = layers.UpSampling2D((2, 2))\n",
    "        self.de5 = layers.Conv2DTranspose(6, (5, 5), activation='relu')\n",
    "        self.de6 = layers.UpSampling2D((2, 2))\n",
    "        self.de7 = layers.Conv2DTranspose(1, (5, 5))\n",
    "\n",
    "        self.L = L\n",
    "        self.opt = opt\n",
    "        self.type = the_type\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # 编码器\n",
    "        x = self.en1(x)\n",
    "        x = self.en2(x)\n",
    "        x = self.en3(x)\n",
    "        x = self.en4(x)\n",
    "        x = self.en5(x)\n",
    "        x = self.en6(x)\n",
    "        h = self.en7(x)\n",
    "        # 获取均值\n",
    "        mu = self.en8(h)\n",
    "        # 获取方差\n",
    "        log_var = self.en9(h)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "    def decoder(self, z):\n",
    "        # 解码器\n",
    "        z = self.de1(z)\n",
    "        z = self.de2(z)\n",
    "        z = tf.reshape(z, (-1, 120))\n",
    "        z = tf.expand_dims(z, 1)\n",
    "        z = tf.expand_dims(z, 1)\n",
    "        z = self.de3(z)\n",
    "        z = self.de4(z)\n",
    "        z = self.de5(z)\n",
    "        z = self.de6(z)\n",
    "        out = self.de7(z)\n",
    "        return out\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # 模型拼装\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "        log_var_ex = tf.tile(tf.expand_dims(log_var, 0), (self.L, 1, 1))\n",
    "        if self.type == 0:\n",
    "            eps = tf.random.normal(log_var_ex.shape)\n",
    "        else:\n",
    "            eps = tf.cast(np.random.standard_cauchy(log_var_ex.shape), dtype=tf.float32)\n",
    "        std = tf.exp(log_var_ex * 0.5)\n",
    "        mu_ex = tf.tile(tf.expand_dims(mu, 0), (self.L, 1, 1))\n",
    "        z = mu_ex + std * eps\n",
    "        z = tf.reshape(z, (-1, z.shape[2]))\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "\n",
    "# 模型构建\n",
    "class VAEDNN(keras.Model):\n",
    "    def __init__(self, z_dim, L, opt, the_type=0):\n",
    "        super(VAEDNN, self).__init__()\n",
    "        # 编码器\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(z_dim)      # 获得均值\n",
    "        self.fc3 = layers.Dense(z_dim)      # 获得均值\n",
    "\n",
    "        # 解码器\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "        self.L = L\n",
    "        self.opt = opt\n",
    "        self.type = the_type\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # 编码器\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        return mu, log_var\n",
    "\n",
    "    def decoder(self, z):\n",
    "        # 解码器\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # 模型拼装\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "        log_var_ex = tf.tile(tf.expand_dims(log_var, 0), (self.L, 1, 1))\n",
    "        if self.type == 0:\n",
    "            eps = tf.random.normal(log_var_ex.shape)\n",
    "        else:\n",
    "            eps = tf.cast(np.random.standard_cauchy(log_var_ex.shape), dtype=tf.float32)\n",
    "        std = tf.exp(log_var_ex * 0.5)\n",
    "        mu_ex = tf.tile(tf.expand_dims(mu, 0), (self.L, 1, 1))\n",
    "        z = mu_ex + std * eps\n",
    "        z = tf.reshape(z, (-1, z.shape[2]))\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "\n",
    "# 误差数据\n",
    "class History():\n",
    "    def __init__(self):\n",
    "        self.kl_divs = []\n",
    "        self.the_loss = []\n",
    "        self.the_rec_loss = []\n",
    "        self.kl_divs_tests = []\n",
    "        self.the_loss_tests = []\n",
    "        self.the_rec_loss_tests = []\n",
    "\n",
    "    def update(self, kl_div, rec_loss, my_loss, kl_div_test, rec_loss_test, my_loss_test):\n",
    "        self.kl_divs.append(kl_div.numpy())\n",
    "        self.the_loss.append(my_loss.numpy())\n",
    "        self.the_rec_loss.append(rec_loss.numpy())\n",
    "        self.kl_divs_tests.append(kl_div_test.numpy())\n",
    "        self.the_loss_tests.append(my_loss_test.numpy())\n",
    "        self.the_rec_loss_tests.append(rec_loss_test.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreatePicture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Auxiliary import mkdir\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 图像存储\n",
    "def save_image(data, name, path):\n",
    "    mkdir(path)\n",
    "    save_img_path = '{}{}.jpg'.format(path, name)\n",
    "    new_img = np.zeros((280, 280))\n",
    "    for index, each_img in enumerate(data[:100]):\n",
    "        row_start = int(index/10) * 28\n",
    "        col_start = (index % 10)*28\n",
    "        new_img[row_start:row_start+28, col_start:col_start+28] = each_img\n",
    "\n",
    "    plt.imsave(save_img_path, new_img, cmap='Greys_r')\n",
    "\n",
    "\n",
    "# 编码解码测试图像\n",
    "def encode_decode_test(x, x_hat, epoch, path):\n",
    "    save_image(x, '{}_label'.format(epoch), path)\n",
    "    x_hat = tf.sigmoid(x_hat)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "    save_image(x_hat, '{}_pre'.format(epoch), path)\n",
    "\n",
    "\n",
    "# 添加噪音测试\n",
    "def noise_test(x, epoch, path, my_model):\n",
    "    x = np.array(x)\n",
    "    save_image(x, '{}_origin'.format(epoch), path)\n",
    "    the_noise = tf.cast(np.random.binomial(1, 0.1, x.shape), dtype=tf.float32)\n",
    "    x = tf.maximum(x, the_noise)\n",
    "    save_image(x, '{}_noise'.format(epoch), path)\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_hat, _, _ = my_model(x)\n",
    "    x_hat = tf.sigmoid(x_hat)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "    save_image(x_hat, '{}_recon'.format(epoch), path)\n",
    "\n",
    "\n",
    "# 添加噪音测试\n",
    "def noise_test2(x, epoch, path, my_model):\n",
    "    x = np.array(x)\n",
    "    save_image(x, '{}_origin'.format(epoch), path)\n",
    "    the_noise = tf.cast(np.random.binomial(1, 0.1, x.shape), dtype=tf.float32)\n",
    "    x = tf.maximum(x, the_noise)\n",
    "    save_image(x, '{}_noise'.format(epoch), path)\n",
    "    x = tf.reshape(x, [-1, 784])\n",
    "    x_hat, _, _ = my_model(x)\n",
    "    x_hat = tf.sigmoid(x_hat)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "    save_image(x_hat, '{}_recon'.format(epoch), path)\n",
    "\n",
    "\n",
    "# 生成新的图像\n",
    "def generate_picture(epoch, path, batchsz, z_dim, my_model, the_type):\n",
    "    if the_type == 0:\n",
    "        z = tf.random.normal((batchsz, z_dim))\n",
    "    else:\n",
    "        z = tf.cast(np.random.standard_cauchy((batchsz, z_dim)), dtype=tf.float32)\n",
    "    logits = my_model.decoder(z)\n",
    "    x_hat = tf.sigmoid(logits)\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() * 255.\n",
    "    save_image(x_hat, '{}_random'.format(epoch), path)\n",
    "\n",
    "\n",
    "# 可视化\n",
    "def graph(history, path, z_dim, batchsz, learn_rate, theta, label):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history.kl_divs, label='train')\n",
    "    plt.plot(history.kl_divs_tests, label='test')\n",
    "    plt.ylabel('KL divergence')\n",
    "    plt.legend()\n",
    "    plt.subplot(132)\n",
    "    plt.plot(history.the_rec_loss, label='train')\n",
    "    plt.plot(history.the_rec_loss_tests, label='test')\n",
    "    plt.ylabel('reconstruction loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(133)\n",
    "    plt.plot(history.the_loss, label='train')\n",
    "    plt.plot(history.the_loss_tests, label='test')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('{}Z{} B{} L{} T{} {}.png'.format(path, z_dim, batchsz, learn_rate, theta, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import History\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from CreatePicture import encode_decode_test, generate_picture, graph, noise_test\n",
    "import random\n",
    "\n",
    "\n",
    "# 损失函数\n",
    "def process(x, L, my_model, theta):\n",
    "    x_hat, mu, log_var = my_model(x)\n",
    "    x_hat = tf.reshape(x_hat, (L, -1, x_hat.shape[1], x_hat.shape[2], x_hat.shape[3]))\n",
    "    # 平方差\n",
    "    #rec_loss = tf.square(tf.tile(tf.expand_dims(x, 0), (L, 1, 1, 1, 1))-x_hat)\n",
    "    # 交叉熵\n",
    "    rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.tile(tf.expand_dims(x, 0), (L, 1, 1, 1, 1)), logits=x_hat)\n",
    "    rec_loss = tf.reduce_mean(rec_loss, 0)\n",
    "    rec_loss = tf.reduce_sum(rec_loss)/x.shape[0]\n",
    "    # KL散度\n",
    "    if my_model.type == 0:\n",
    "        kl_div = -0.5 * (log_var + 1 - mu ** 2 - tf.exp(log_var))\n",
    "    else:\n",
    "        kl_div = -0.5 * log_var - 2 * tf.math.log(2.) + tf.math.log((mu**2 - tf.exp(log_var) + 1)**2 + 4*tf.exp(log_var) * mu**2) - tf.math.log((tf.exp(0.5*log_var)-1)**2 + mu**2)\n",
    "    kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "\n",
    "    # 两个误差结合\n",
    "    my_loss = rec_loss + theta * kl_div\n",
    "    return kl_div, rec_loss, my_loss, x_hat\n",
    "\n",
    "\n",
    "# 运行模型\n",
    "def run_model(epochs, train_db, x_test, path, batchsz, z_dim, my_model, theta, learn_rate, label):\n",
    "    history = History()\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1\n",
    "        for x in train_db:\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "            with tf.GradientTape() as tape:\n",
    "                kl_div, rec_loss, my_loss, _ = process(x, my_model.L, my_model, theta)\n",
    "            grads = tape.gradient(my_loss, my_model.trainable_variables)\n",
    "            my_model.opt.apply_gradients(zip(grads, my_model.trainable_variables))\n",
    "        if epoch % 5 == 0:\n",
    "            # 测试模型\n",
    "            x_0 = random.sample(list(x_test), 100)\n",
    "            x = tf.reshape(x_0, [-1, 28, 28, 1])\n",
    "            kl_div_test, rec_loss_test, my_loss_test, x_hat_logits = process(x, 1, my_model, theta)\n",
    "            print(\"epoch:{:<3d} (train) KL divergence:{:.2f} reconstruction loss:{:.2f} loss:{:.2f}\".format(epoch, kl_div, rec_loss, my_loss))\n",
    "            print(\"          (test)  KL divergence:{:.2f} reconstruction loss:{:.2f} loss:{:.2f}\".format(kl_div_test, rec_loss_test, my_loss_test))\n",
    "            history.update(kl_div, rec_loss, my_loss, kl_div_test, rec_loss_test, my_loss_test)\n",
    "            if epoch % 5 == 0:\n",
    "                # 编码解码图片\n",
    "                encode_decode_test(x_0, x_hat_logits, epoch, path)\n",
    "                # 噪音图片测试\n",
    "                noise_test(x_0, epoch, path, my_model)\n",
    "                # 用解码器生成图片\n",
    "                generate_picture(epoch, path, batchsz, z_dim, my_model, my_model.type)\n",
    "        if epoch % 100 == 0:\n",
    "            # 可视化\n",
    "            graph(history, path, z_dim, batchsz, learn_rate, theta, label)\n",
    "            # 存储数据\n",
    "            frame = pd.DataFrame({'kl_divs': history.kl_divs, 'the_loss': history.the_loss, 'the_rec_loss': history.the_rec_loss, 'kl_divs_tests': history.kl_divs_tests, 'the_loss_tests': history.the_loss_tests, 'the_rec_loss_tests': history.the_rec_loss_tests})\n",
    "            frame.to_csv('{}Z{} B{} L{} T{} CNN Cauchy.csv'.format(path, z_dim, batchsz, learn_rate, theta), sep=',')\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE tensorflow CNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VAECNN\n",
    "from RunCNN import run_model\n",
    "import numpy as np\n",
    "from Auxiliary import GPU\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "def data(data):\n",
    "    (x_train, _), (x_test, _) = data.load_data()\n",
    "    x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "    train_db = tf.data.Dataset.from_tensor_slices(x_train).shuffle(batchsz * 5).batch(batchsz)\n",
    "    return train_db, x_test\n",
    "\n",
    "\n",
    "label = Path(__file__).name\n",
    "path = './data/{}/27C/'.format(label)\n",
    "# 超参数\n",
    "z_dim = 10\n",
    "batchsz = 512\n",
    "learn_rate = 1e-4\n",
    "theta = 1\n",
    "epochs = 500\n",
    "L = 1\n",
    "\n",
    "GPU()\n",
    "mnist = tf.keras.datasets.mnist\n",
    "train_db, x_test = data(mnist)\n",
    "my_model = VAECNN(z_dim, L, opt=tf.optimizers.Adam(learn_rate))\n",
    "history = run_model(epochs, train_db, x_test, path, batchsz, z_dim, my_model, theta, learn_rate, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE tensorflow DNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VAEDNN\n",
    "from RunDNN import run_model\n",
    "import numpy as np\n",
    "from Auxiliary import GPU\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "def data(data):\n",
    "    (x_train, _), (x_test, _) = data.load_data()\n",
    "    x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n",
    "    train_db = tf.data.Dataset.from_tensor_slices(x_train).shuffle(batchsz * 5).batch(batchsz)\n",
    "    return train_db, x_test\n",
    "\n",
    "\n",
    "label = Path(__file__).name\n",
    "path = './data/{}/6/'.format(label)\n",
    "# 超参数\n",
    "z_dim = 10\n",
    "batchsz = 512\n",
    "learn_rate = 1e-4\n",
    "theta = 1\n",
    "epochs = 500\n",
    "L = 1\n",
    "\n",
    "GPU()\n",
    "mnist = tf.keras.datasets.mnist\n",
    "train_db, x_test = data(mnist)\n",
    "my_model = VAEDNN(z_dim, L, opt=tf.optimizers.Adam(learn_rate))\n",
    "run_model(epochs, train_db, x_test, path, batchsz, z_dim, my_model, theta, learn_rate, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
